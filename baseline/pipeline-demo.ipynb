{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b95fcbe-4085-4cb6-8fde-7c4601826387",
   "metadata": {
    "id": "mlhq3qfjqFO2"
   },
   "source": [
    "# Model Risk Pipeline\n",
    "\n",
    "_Initial commit: Anton Markov, 19 November 2021_\n",
    "\n",
    "Основная цель данного ноутбука — построить базовую структуру пайплайна с учетом оптимизации гиперпараметров.\n",
    "\n",
    "Реализована оптимизация через `hyperopt`, в будущем возможно поддержка иных библиотек.\n",
    "\n",
    "__Входные данные:__\n",
    "\n",
    "1. Датасет\n",
    "2. Модель\n",
    "3. Список модулей, которые могут оптимизироваться в качества гиперпараметра\n",
    "\n",
    "__Исходящие данные:__\n",
    "\n",
    "1. Оптимальный набор модулей, согласно `hyperopt`\n",
    "2. Параметры обученной оптимальной модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9abbb-ee51-484f-96b8-192c3474e7ff",
   "metadata": {},
   "source": [
    "## 1. Technicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144269a-e212-4862-bccc-765c1e35f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import umap\n",
    "\n",
    "from sklearn import datasets, metrics, model_selection\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from hyperopt import hp\n",
    "# from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# import umap # does not work for me, since I installed umap instaed of umap-learn by mistake\n",
    "\n",
    "# for HyperOpt class\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "# import catboost as ctb\n",
    "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "\n",
    "# новый пакет!\n",
    "from feature_engine.encoding import WoEEncoder\n",
    "from feature_engine.creation import CombineWithReferenceFeature\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "from typing import List, Union\n",
    "from feature_engine.encoding.base_encoder import BaseCategoricalTransformer\n",
    "from feature_engine.validation import _return_tags\n",
    "from feature_engine.variable_manipulation import _check_input_parameter_variables\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from feature_engine.selection  import SelectByShuffling\n",
    "from feature_engine.selection  import RecursiveFeatureAddition\n",
    "from feature_engine.selection  import SmartCorrelatedSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2550a6d-604b-4c7c-9929-0d9365b10eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac53db-2b8f-45b7-b253-509fa7045915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(y, y_pred):\n",
    "    res = roc_auc_score(y, y_pred) * 2 - 1\n",
    "    print(f\"Gini: {res}\")\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6d70f-f723-4f54-bf97-58153affb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_params(params, pipe):\n",
    "    '''\n",
    "    From all input parameters filter only\n",
    "    those that are relevant for the current\n",
    "    pipeline\n",
    "    '''\n",
    "    pipe_steps = list(pipe.named_steps.keys())\n",
    "    params_keys = list(params.keys())\n",
    "    \n",
    "    return {\n",
    "        key: params[key]\n",
    "        for key in params_keys\n",
    "        if key.split('__')[0] in pipe_steps\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555e819-4f9b-481e-a875-7f11be4320f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pipe(steps_dict, modules):\n",
    "    '''\n",
    "    Construct a pipeline given structure\n",
    "    '''\n",
    "    return [(steps_dict[s], modules[steps_dict[s]]) for s in steps_dict if steps_dict[s] != 'skip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cfd20-67df-4759-9f81-ecf91791f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipeHPOpt(object):\n",
    "    '''\n",
    "    Класс PipeHPOpt — Pipeline with hyperparameter optimisation\n",
    "    using hyperopt — нацелен на оптимизацию пайплайна как с точки\n",
    "    зрения входящих в него модулей, так и гиперпараметров каждого\n",
    "    из модулей\n",
    "    '''  \n",
    "    def __init__(self, X, y, modules, mode='kfold', n_folds = 5, test_size=.33, seed=42):\n",
    "        '''   \n",
    "        _inputs:\n",
    "        X — train dataset\n",
    "        y — train targets\n",
    "        modules — dict of all modules that might potentially be included into\n",
    "            the pipeline\n",
    "        mode — wither \"kfold\" or \"valid\" (error if other) — sets if X, y will\n",
    "            be subdivided into k cross-validation samples or train/test samples,\n",
    "            respectively. \"kfold\" is default. Key advantage of valid: it returns\n",
    "            the optimal model; in \"kfold\" mode the model should be retrained with\n",
    "            optimal hyperparameters\n",
    "        n_folds — number of folds at cross-validation (5 is default). Applied\n",
    "            only if mode = \"kfold\". Warning added\n",
    "        test_size — test sample % (.33 is default). Applied only if mode = \"valid\". \n",
    "            Warning added\n",
    "        seed — random seed (42 is default)\n",
    "        '''\n",
    "        if (mode != 'kfold') & (mode != 'valid'):\n",
    "            raise ValueError(\"Choose mode 'kfold' or 'valid'\")\n",
    "        if (mode == 'valid') & (n_folds != 5):\n",
    "            import warnings\n",
    "            warnings.warn(\"Non-default n_folds won't be used since mode == valid!\")\n",
    "        if (mode == 'kfold') & (test_size != .33):\n",
    "            import warnings\n",
    "            warnings.warn(\"Non-default test_size won't be used since mode == kfold!\")\n",
    "            \n",
    "        self.X       = X\n",
    "        self.y       = y\n",
    "        self.mode    = mode\n",
    "        self.n_folds = n_folds\n",
    "        self.seed    = seed\n",
    "        self.modules = modules\n",
    "        \n",
    "        if mode == 'valid':\n",
    "            self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=seed\n",
    "            )\n",
    "\n",
    "    def process(self, space, trials, algo, max_evals, fn_name='_pipe'):\n",
    "        '''\n",
    "        _inputs: TBD\n",
    "        \n",
    "        _output:\n",
    "        result: hyperopt weird object of the optimal model representation\n",
    "        trials: info on each of the hyperopt trials\n",
    "        '''\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL,\n",
    "                    'exception': str(e)}\n",
    "        self.result = result\n",
    "        self.trials = trials\n",
    "        return result, trials\n",
    "\n",
    "    \n",
    "    def get_best_params(self):\n",
    "        return self.trials.results[np.argmin([r['loss'] for r in trials.results])]['params']\n",
    "    \n",
    "    def get_best_model(self):\n",
    "        para = self.get_best_params()\n",
    "        pipe_steps = [(para['pipe_params'][i], modules[para['pipe_params'][i]]) for i in para['pipe_params'] if para['pipe_params'][i] != 'skip']\n",
    "        reg = Pipeline(pipe_steps)\n",
    "        for p in pipe_para['set_params']:\n",
    "            try:\n",
    "                reg.set_params({p: para[p]})\n",
    "            except:\n",
    "                pass # repetition, not DRY, think how to delete\n",
    "        return reg.fit(self.X, self.y)\n",
    "    \n",
    "    def _pipe(self, para):\n",
    "        # print(para)\n",
    "        pipe_steps = [(para['pipe_params'][i], modules[para['pipe_params'][i]]) for i in para['pipe_params'] if para['pipe_params'][i] != 'skip']\n",
    "        reg = Pipeline(pipe_steps)\n",
    "        for p in pipe_para['set_params']:\n",
    "            try:\n",
    "                reg.set_params({p: para[p]})\n",
    "            except:\n",
    "                pass\n",
    "        if self.mode == 'kfold':\n",
    "            return self._train_reg_kfold(reg, para)\n",
    "        elif self.mode == 'valid':\n",
    "            return self._train_reg_valid(reg, para)\n",
    "\n",
    "    def _train_reg_valid(self, reg, para):\n",
    "        reg.fit(self.x_train, self.y_train)\n",
    "        pred = reg.predict_proba(self.x_test)[:, 1]\n",
    "        loss = para['loss_func'](self.y_test, pred)\n",
    "        return {'loss': loss, 'model': reg, 'params': para, 'status': STATUS_OK}\n",
    "    \n",
    "    def _train_reg_kfold(self, reg, para):\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=self.seed)\n",
    "        losses = []\n",
    "        for train_index, test_index in kf.split(self.X):\n",
    "            X_split_train, X_split_test = self.X.iloc[train_index, :], self.X.iloc[test_index, :]\n",
    "            y_split_train, y_split_test = self.y.iloc[train_index, ],  self.y.iloc[test_index, ]\n",
    "            reg.fit(X_split_train, y_split_train)\n",
    "            pred = reg.predict_proba(X_split_test)[:, 1]\n",
    "            loss = para['loss_func'](y_split_test, pred)\n",
    "            losses.append(loss)\n",
    "        return {'loss': np.mean(losses), 'params': para, 'status': STATUS_OK}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020c325-11af-4cfc-9d61-5d2378598fdd",
   "metadata": {},
   "source": [
    "## 2. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b36c16-1aa8-41a0-8462-74cfbe7fc49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet('../datasets/01_german/samples/X_train.parquet')\n",
    "y_train = pd.read_parquet('../datasets/01_german/samples/y_train.parquet').target\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "\n",
    "X_test  = pd.read_parquet('../datasets/01_german/samples/X_test.parquet')\n",
    "y_test  = pd.read_parquet('../datasets/01_german/samples/y_test.parquet').target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff79df1-8826-4f05-918e-03652f8a036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/01_german/factors.json') as json_file:\n",
    "    factors_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bcf567-6fb6-455c-8ec6-09028e11e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_dict['cat_vals']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58540fea-e6ce-462c-8427-30f443126065",
   "metadata": {},
   "source": [
    "## 3. Define Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773fea9-9067-4d91-a30d-4594eb5a3716",
   "metadata": {},
   "source": [
    "All the modules that might be part of the pipeline should be defined below (or import them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c5e57-7b87-420b-b5a8-8bcedade03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineWithReferenceFeature_adj():\n",
    "    \"\"\"\n",
    "    Обертка вокруг CombineWithReferenceFeature()\n",
    "    Позволяет не устанавливать параметры\n",
    "    + variables_to_combine\n",
    "    + reference_variables\n",
    "    заранее (иначе не будет работать с OneHotEncoder\n",
    "    и прочими преобразователями данных, а делать это при .fit()\n",
    "    \"\"\"\n",
    "    def __init__(self, operations):\n",
    "        self.operations = operations\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.combinator = CombineWithReferenceFeature(\n",
    "            variables_to_combine = list(X.columns),\n",
    "            reference_variables = list(X.columns),\n",
    "            operations = self.operations\n",
    "        )\n",
    "        self.combinator.fit(X, y)\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return(self.combinator.transform(X))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4719df0-4d13-4177-b6f8-1af8f91ffc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionReducer():\n",
    "    \"\"\"\n",
    "    Обертка нужна:\n",
    "    1. Чтобы не заменять фичи, а добавлять их к исходному df\n",
    "    2. Для PCA ouput = np.array, требуется заменить на pd.DataFrame \n",
    "    \"\"\"\n",
    "    def __init__(self, gen_class, **kwargs):\n",
    "        self.reducer = gen_class(**kwargs)\n",
    "        # self.reducer.set_params()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.reducer.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # potentially \n",
    "        return pd.concat([X, pd.DataFrame(self.reducer.transform(X), index = X.index)], axis=1)\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        self.reducer.set_params(**kwargs)\n",
    "        return self     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260b472-260b-4716-a2b3-eda7e0910baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Union\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# from feature_engine.encoding.base_encoder import BaseCategoricalTransformer\n",
    "# from feature_engine.validation import _return_tags\n",
    "# from feature_engine.variable_manipulation import _check_input_parameter_variables\n",
    "\n",
    "class WoEEncoder_adj(BaseCategoricalTransformer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        variables: Union[None, int, str, List[Union[str, int]]] = None,\n",
    "        ignore_format: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        if not isinstance(ignore_format, bool):\n",
    "            raise ValueError(\"ignore_format takes only booleans True and False\")\n",
    "\n",
    "        self.variables = _check_input_parameter_variables(variables)\n",
    "        self.ignore_format = ignore_format\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Learn the WoE.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas dataframe of shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "            Can be the entire dataframe, not just the categorical variables.\n",
    "        y: pandas series.\n",
    "            Target, must be binary.\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self._check_fit_input_and_variables(X)\n",
    "\n",
    "        if not isinstance(y, pd.Series):\n",
    "            y = pd.Series(y)\n",
    "\n",
    "        # check that y is binary\n",
    "        if y.nunique() != 2:\n",
    "            raise ValueError(\n",
    "                \"This encoder is designed for binary classification. The target \"\n",
    "                \"used has more than 2 unique values.\"\n",
    "            )\n",
    "\n",
    "        temp = pd.concat([X, y], axis=1)\n",
    "        temp.columns = list(X.columns) + [\"target\"]\n",
    "\n",
    "        # if target does not have values 0 and 1, we need to remap, to be able to\n",
    "        # compute the averages.\n",
    "        if any(x for x in y.unique() if x not in [0, 1]):\n",
    "            temp[\"target\"] = np.where(temp[\"target\"] == y.unique()[0], 0, 1)\n",
    "\n",
    "        self.encoder_dict_ = {}\n",
    "\n",
    "        total_pos = temp[\"target\"].sum()\n",
    "        total_neg = len(temp) - total_pos\n",
    "        temp[\"non_target\"] = np.where(temp[\"target\"] == 1, 0, 1)\n",
    "\n",
    "        for var in self.variables_:\n",
    "            pos = (temp.groupby([var])[\"target\"].sum() + .5) / total_pos\n",
    "            neg = (temp.groupby([var])[\"non_target\"].sum() + .5) / total_neg\n",
    "\n",
    "            t = pd.concat([pos, neg], axis=1)\n",
    "            t[\"woe\"] = np.log(t[\"target\"] / t[\"non_target\"])\n",
    "\n",
    "            # we make an adjustment to override this error\n",
    "            # if (\n",
    "            #     not t.loc[t[\"target\"] == 0, :].empty\n",
    "            #     or not t.loc[t[\"non_target\"] == 0, :].empty\n",
    "            # ):\n",
    "            #     raise ValueError(\n",
    "            #         \"The proportion of one of the classes for a category in \"\n",
    "            #         \"variable {} is zero, and log of zero is not defined\".format(var)\n",
    "            #     )\n",
    "\n",
    "            self.encoder_dict_[var] = t[\"woe\"].to_dict()\n",
    "\n",
    "        self._check_encoding_dictionary()\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Ugly work around to import the docstring for Sphinx, otherwise not necessary\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = super().transform(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    transform.__doc__ = BaseCategoricalTransformer.transform.__doc__\n",
    "\n",
    "    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = super().inverse_transform(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    inverse_transform.__doc__ = BaseCategoricalTransformer.inverse_transform.__doc__\n",
    "\n",
    "    def _more_tags(self):\n",
    "        tags_dict = _return_tags()\n",
    "        # in the current format, the tests are performed using continuous np.arrays\n",
    "        # this means that when we encode some of the values, the denominator is 0\n",
    "        # and this the transformer raises an error, and the test fails.\n",
    "        # For this reason, most sklearn transformers will fail. And it has nothing to\n",
    "        # do with the class not being compatible, it is just that the inputs passed\n",
    "        # are not suitable\n",
    "        tags_dict[\"_skip_test\"] = True\n",
    "        return tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc81ed-dce6-4a74-a8cd-549a82b69ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WoE_module = WoEEncoder_adj(variables = factors_dict['cat_vals'])\n",
    "\n",
    "OneHot_module = OneHotEncoder(variables = factors_dict['cat_vals'])\n",
    "\n",
    "PCA_module = DimensionReducer(\n",
    "    gen_class = sklearn.decomposition.PCA,\n",
    "    n_components = 2,    # сколько оставить компонентов; по дефолту - все\n",
    "    whiten = False,      # отключаем whitening - декорреляцию фичей\n",
    "    svd_solver = \"full\", # детали SVD преобразования, за подробностями см. доки\n",
    ")\n",
    "\n",
    "kPCA_module = DimensionReducer(\n",
    "    gen_class = sklearn.decomposition.KernelPCA,\n",
    "    n_components = 8,  # сколько оставить компонентов; по дефолту - все\n",
    "    kernel = \"linear\", # ядро. По дфеолту линейное. Можно сделать своё, но тогда его нужно предварительно вычислить отдельно,\n",
    "                       # поставить kernel = \"precomputed\" и передать уже вычисленное ядро в качестве X\n",
    "    degree = 3,        # степень полинома для некоторых типов ядер. Важный параметр для тьюнинга, но сильно напрягает процессор\n",
    "    n_jobs = -1        # объект умеет быть многопоточным! -1 займет все ядра\n",
    ")\n",
    "\n",
    "Isomap_module = DimensionReducer(\n",
    "    gen_class = sklearn.manifold.Isomap,\n",
    "    n_neighbors = 5, #количество соседей при вычислении KNN. Основной гиперпараметр, кстати (!!!)\n",
    "    n_components = 2,  #сколько оставить компонент; по дефолту - 2\n",
    "    path_method = \"auto\", #алгоритм, который вычисляет кратчайший путь. Варианты см. на странице функции. Этот подбирает сам.\n",
    "    neighbors_algorithm = \"auto\", #алгоритм, который ищет соседей. Инстанс класса NearestNeighbours\n",
    "    n_jobs = -1 #объект умеет быть многопоточным! -1 займет все ядра\n",
    ")\n",
    "\n",
    "UMAP_module = DimensionReducer(\n",
    "    gen_class = umap.UMAP,\n",
    "    n_neighbors = 5,  # количество соседей при вычислении KNN. Основной гиперпараметр, кстати (!!!)\n",
    "    n_components = 2, # сколько оставить компонентов; по дефолту - 2\n",
    "    min_dist = 0.1    # минимальная дистанция, которую можно сохранять между точками в получающемся пространстве. \n",
    "    # Гиперпараметр. При увеличении начинает лучше улавливать общую структуру, но хуже - локальную\n",
    ")\n",
    "\n",
    "CombWRef_module = CombineWithReferenceFeature_adj(\n",
    "    operations = ['mul']\n",
    ")\n",
    "\n",
    "lgbm_mdl = LGBMClassifier(\n",
    "    num_leaves = 10,\n",
    "    learning_rate = .1,\n",
    "    reg_alpha = 8,\n",
    "    reg_lambda = 8,\n",
    "    random_state = seed\n",
    ")\n",
    "\n",
    "# Tackling imbalances in target\n",
    "RUS_module    = RandomUnderSampler(random_state = seed)\n",
    "ROS_module    = RandomOverSampler(random_state = seed)\n",
    "SMOTE_module  = SMOTE(random_state = seed)\n",
    "ADASYN_module = ADASYN(random_state = seed)\n",
    "\n",
    "# feature selection\n",
    "SeqFearSel_module = SequentialFeatureSelector(\n",
    "    estimator  = lgbm_mdl,  \n",
    "    # k_features = 5,                                                  \n",
    "    forward    = True,                                                  \n",
    "    floating   = True,                                                \n",
    "    verbose    = 0,\n",
    "    cv         = 5\n",
    ")\n",
    "RecFeatAdd_module = RecursiveFeatureAddition(\n",
    "    lgbm_mdl,\n",
    "    threshold = 0.005\n",
    ")\n",
    "# SelShuffl_module = SelectByShuffling(\n",
    "#     estimator = lgbm_mdl,\n",
    "#     # variables=X.columns.to_list(),                                      # можно задать подмножество\n",
    "#     scoring='roc_auc',                                                  # метрика\n",
    "#     threshold=0.01,                                                     # порог ее снижения\n",
    "#     cv=5,\n",
    "#     random_state=42\n",
    "# )\n",
    "SmartSel_module = SmartCorrelatedSelection(\n",
    "    # variables=X.columns.to_list(),\n",
    "    method=\"pearson\",                # можно взять свою функцию\n",
    "    threshold=0.3,                   # порог корреляции\n",
    "    selection_method=\"variance\",     # из коррелирующих групп выбираем признак с наиб дисперсией\n",
    "    estimator=None,                  # понадобится для selection_method=\"model_performance\"        \n",
    "    cv=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12046c50-6419-41a5-add4-99f4329eda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = {\n",
    "    'WoE':         WoE_module,\n",
    "    'OneHot':      OneHot_module,\n",
    "    'PCA':         PCA_module,\n",
    "    'kPCA':        kPCA_module,\n",
    "    'Isomap':      Isomap_module,\n",
    "    'UMAP':        UMAP_module,\n",
    "    'CombWRef':    CombWRef_module,\n",
    "    'RecFeatAdd':  RecFeatAdd_module,\n",
    "    'lgbm':        lgbm_mdl,\n",
    "    'RUS':         RUS_module,      \n",
    "    'ROS':         ROS_module,      \n",
    "    'SMOTE':       SMOTE_module,  \n",
    "    'ADASYN':      ADASYN_module,\n",
    "    'SeqFearSel':  SeqFearSel_module,\n",
    "    'RecFeatAdd':  RecFeatAdd_module,\n",
    "    # 'SelShuffl':   SelShuffl_module,\n",
    "    'SmartSel':    SmartSel_module    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9191776-4f78-43dc-aa59-162abcb164d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Define Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb5e9c-b033-4ce9-a5eb-113692201e73",
   "metadata": {},
   "source": [
    "Статья с примером [здесь](https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc113da-2803-4158-b23f-685890180303",
   "metadata": {},
   "source": [
    "### 4.1. Структура Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fcacdc-9228-4db2-b546-25700f07e8e6",
   "metadata": {},
   "source": [
    "Определим структуру самого пайплайна. Словесное описание:\n",
    "    \n",
    "1. Энкодинг категориальных переменных:\n",
    "    + OneHotEncoder\n",
    "    + WoE\n",
    "3. Feature Engineering:\n",
    "    + PCA\n",
    "    + Kernel PCA\n",
    "    + Isomap\n",
    "    + UMAP\n",
    "    + Combine with Reference (feature multiplication)\n",
    "    + _отсутствует_\n",
    "4. Feature Selection:\n",
    "    + RecursiveFeatureAddition\n",
    "    + SequentialFeatureSelector\n",
    "    + SmartCorrelatedSelection\n",
    "    + _отсутствует_\n",
    "4. Resampling:\n",
    "    + Randomised Undersampling (RUS)\n",
    "    + Randomised Oversampling  (ROS)\n",
    "    + Synthetic Minority Oversampling Technique (SMOTE)\n",
    "    + Adaptive Synthetic (ADASYN)\n",
    "    + _отсутствует_\n",
    "5. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda1a09-9ad5-480d-b1ef-7ecdeaa21774",
   "metadata": {},
   "source": [
    "А так это будет выражаться в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3a5d1-d094-4acd-a218-a6e3839928eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    # 'missing_vals': \n",
    "    'cat_encoding':  hp.choice('cat_encoding', ['OneHot', 'WoE']), # , 'woe' пропустить нельзя из-за наличия кат. пер-х\n",
    "    'imbalance':     hp.choice('imbalance',    ['skip', 'RUS', 'ROS', 'SMOTE', 'ADASYN']),\n",
    "    'feat_eng':      hp.choice('feat_eng',     ['skip', 'PCA', 'kPCA', 'Isomap', 'UMAP']), # , 'CombWRef' # удалил, т.к. долго считается\n",
    "    # 'feat_filter':   hp.choice() -- unstable, low quality (SHAP, IV, feat_importance, ...) \n",
    "    'feat_sel':      hp.choice('feat_sel',     ['skip', 'SeqFearSel', 'RecFeatAdd', 'SmartSel']), # 'SelShuffl' is omitted, since it might drop all Xs\n",
    "    'lgbm':          'lgbm'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9529da0-a54b-46ee-b43b-b6f5a14b4705",
   "metadata": {},
   "source": [
    "Заметим, что 'skip' позволяет игнорировать соответствующий шаг в пайплайне. Названия типа `\"onehot\"` должны совпадать с названиями в словаре `modules`, который мы определили на Шаге 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e3b4f-8611-485d-9b80-fd347684a1e2",
   "metadata": {},
   "source": [
    "### 4.2. Гиперпараметры модулей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1b7ab-92ba-4154-9936-c218e70f8dbf",
   "metadata": {},
   "source": [
    "В следующий словарь добавляем гиперпараметры каждого из модулей, которые мы хотим оптимизировать. Названия строятся следующи образом:\n",
    "\n",
    "`<Название модуля>__<название параметра>`\n",
    "\n",
    "Например, чтобы задать параметр `num_leaves` модуля lgbm, трубуется добваить значение с ключем `lgbm__num_leaves`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b710156-03b3-4e15-9b8f-3d1056037741",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_params = {\n",
    "    # OneHotEncoder does not need hyperparams\n",
    "    # RecFeatAdd might be redefined to receive a correct estimator\n",
    "    # PCA\n",
    "    'DimRed__PCA__n_components':      hp.choice('PCA__n_components',      np.arange(2, 11)),\n",
    "    'DimRed__PCA__whiten':            hp.choice('PCA__whiten',            [True, False]),\n",
    "    'DimRed__PCA__svd_solver':        hp.choice('PCA__svd_solver',        ['full', 'arpack', 'auto', 'randomized']),\n",
    "    \n",
    "    # kPCA\n",
    "    'DimRed__kPCA__n_components':     hp.choice('kPCA__n_components',     np.arange(5, 11)),\n",
    "    'DimRed__kPCA__kernel':           hp.choice('kPCA__kernel',           ['linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed']),\n",
    "    \n",
    "    # Isomap\n",
    "    'DimRed__Isomap__n_neighbors':    hp.choice('Isomap__n_neighbors',    np.arange(2, 11)),\n",
    "    'DimRed__Isomap__n_components':   hp.choice('Isomap__n_components',   np.arange(2, 5)),\n",
    "    'DimRed__Isomap__path_method':    hp.choice('Isomap__path_method',    ['auto', 'FW', 'D']),\n",
    "    \n",
    "    # UMAP\n",
    "    'DimRed__UMAP__n_neighbors':      hp.choice('UMAP__n_neighbors',      np.arange(2, 11)),\n",
    "    'DimRed__UMAP__n_components':     hp.choice('UMAP__n_components',     np.arange(2, 11)),\n",
    "    'DimRed__UMAP__min_dist':         hp.choice('UMAP__min_dist',         np.arange(0.05, 1, 0.05)),\n",
    "    \n",
    "    # LightGBM\n",
    "    'lgbm__learning_rate':    hp.choice('lgbm__learning_rate',    np.arange(0.05, 0.31, 0.05)),\n",
    "    'lgbm__num_leaves':       hp.choice('lgbm__num_leaves',       np.arange(5, 16, 1, dtype=int)),\n",
    "    'lgbm__reg_alpha':        hp.choice('lgbm__reg_alpha',        np.arange(0, 16, 1, dtype=int)),\n",
    "    'lgbm__reg_lambda':       hp.choice('lgbm__reg_lambda',       np.arange(0, 16, 1, dtype=int)),\n",
    "    'lgbm__n_estimators':     100\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e087a-0d3e-45ab-95a4-51dfa65d4fe0",
   "metadata": {},
   "source": [
    "Чтобы параметры можно было оптимизировать, модули должны иметь метод `.set_params()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0394d46-ceb8-4d7c-8d9f-34ae8a58c8a8",
   "metadata": {},
   "source": [
    "### 4.3. Подстановка оптимизационной задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d558b4-e575-4e78-9999-72581f265261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# technicals — define minimization task\n",
    "pipe_para = dict()\n",
    "pipe_para['pipe_params']    = pipe_params\n",
    "pipe_para['set_params']     = set_params\n",
    "pipe_para['loss_func']      = lambda y, pred: -sklearn.metrics.roc_auc_score(y, pred)\n",
    "# pipe_para['loss_func']      = lambda y, pred: -sklearn.metrics.log_loss(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35572ab5-126d-4c78-a757-5a423def797f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "hpoptimizer = PipeHPOpt(X_train, y_train, modules=modules, mode='kfold', n_folds = 5, seed=seed)\n",
    "lgb_opt, trials = hpoptimizer.process(space=pipe_para, trials=Trials(), algo=tpe.suggest, max_evals=1000)\n",
    "\n",
    "# hpoptimizer = PipeStructHPOpt(X_train, y_train, modules, space_params=set_params, mode='kfold', n_folds = 5, seed=seed)\n",
    "# lgb_opt, trials = hpoptimizer.process(space_steps=steps, trials=Trials(), algo=tpe.suggest, max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4f5de-fb1e-4ecd-ae57-bbb5da1f1c76",
   "metadata": {},
   "source": [
    "### 4.4. Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b7ca6-89f8-46a5-8b4c-78a33730fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpoptimizer.get_best_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e6259-9a6b-48b3-be5e-afddd7b058a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mdl = hpoptimizer.get_best_model()\n",
    "best_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8832c-8df3-4acb-8535-11dc6a9523ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gini(y_train, best_mdl.predict_proba(X_train)[:, 1])\n",
    "Gini(y_test, best_mdl.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969a161-a175-454f-9b2b-83d2c74129a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[r['loss'] for r in trials.results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fb6ef-47c1-430b-b686-afe89e47b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array([r['loss'] for r in trials.results]))\n",
    "plt.title('Hyperopt: loss function dynamics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average loss (-AUC) on k cross-validation samples')\n",
    "plt.show()\n",
    "plt.savefig('loss_dynamics_hp_1000_tpe_suggest.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maa-automl",
   "language": "python",
   "name": "maa-automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
